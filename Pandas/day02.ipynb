{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientists = pd.read_csv('scientists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Rosaline Franklin\n",
       "1          William Gosset\n",
       "2    Florence Nightingale\n",
       "3             Marie Curie\n",
       "4           Rachel Carson\n",
       "5               John Snow\n",
       "6             Alan Turing\n",
       "7            Johann Gauss\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = scientists['Name']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.to_pickle('sci_names.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 전체를 피클로 저장할 수 있다 \n",
    "scientists.to_pickle('sci_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Rosaline Franklin\n",
       "1          William Gosset\n",
       "2    Florence Nightingale\n",
       "3             Marie Curie\n",
       "4           Rachel Carson\n",
       "5               John Snow\n",
       "6             Alan Turing\n",
       "7            Johann Gauss\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 피클 읽어오기\n",
    "sci_names_from_pickle = pd.read_pickle('sci_names.pickle')\n",
    "sci_names_from_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Born</th>\n",
       "      <th>Died</th>\n",
       "      <th>Age</th>\n",
       "      <th>Occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rosaline Franklin</td>\n",
       "      <td>1920-07-25</td>\n",
       "      <td>1958-04-16</td>\n",
       "      <td>37</td>\n",
       "      <td>Chemist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>William Gosset</td>\n",
       "      <td>1876-06-13</td>\n",
       "      <td>1937-10-16</td>\n",
       "      <td>61</td>\n",
       "      <td>Statistician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florence Nightingale</td>\n",
       "      <td>1820-05-12</td>\n",
       "      <td>1910-08-13</td>\n",
       "      <td>90</td>\n",
       "      <td>Nurse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Marie Curie</td>\n",
       "      <td>1867-11-07</td>\n",
       "      <td>1934-07-04</td>\n",
       "      <td>66</td>\n",
       "      <td>Chemist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rachel Carson</td>\n",
       "      <td>1907-05-27</td>\n",
       "      <td>1964-04-14</td>\n",
       "      <td>56</td>\n",
       "      <td>Biologist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>John Snow</td>\n",
       "      <td>1813-03-15</td>\n",
       "      <td>1858-06-16</td>\n",
       "      <td>45</td>\n",
       "      <td>Physician</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>1912-06-23</td>\n",
       "      <td>1954-06-07</td>\n",
       "      <td>41</td>\n",
       "      <td>Computer Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Johann Gauss</td>\n",
       "      <td>1777-04-30</td>\n",
       "      <td>1855-02-23</td>\n",
       "      <td>77</td>\n",
       "      <td>Mathematician</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name        Born        Died  Age          Occupation\n",
       "0     Rosaline Franklin  1920-07-25  1958-04-16   37             Chemist\n",
       "1        William Gosset  1876-06-13  1937-10-16   61        Statistician\n",
       "2  Florence Nightingale  1820-05-12  1910-08-13   90               Nurse\n",
       "3           Marie Curie  1867-11-07  1934-07-04   66             Chemist\n",
       "4         Rachel Carson  1907-05-27  1964-04-14   56           Biologist\n",
       "5             John Snow  1813-03-15  1858-06-16   45           Physician\n",
       "6           Alan Turing  1912-06-23  1954-06-07   41  Computer Scientist\n",
       "7          Johann Gauss  1777-04-30  1855-02-23   77       Mathematician"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sci_df_from_pickle = pd.read_pickle('sci_all.pickle')\n",
    "sci_df_from_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Rosaline Franklin\n",
       "1          William Gosset\n",
       "2    Florence Nightingale\n",
       "3             Marie Curie\n",
       "4           Rachel Carson\n",
       "5               John Snow\n",
       "6             Alan Turing\n",
       "7            Johann Gauss\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV, TSV 로 저장하기\n",
    "# CSV 는 데이터를 쉼표/콤마로 구분하여 저장한 파일\n",
    "# TSV 는 데이터를 탭으로 구분하여 저장한 파일\n",
    "# 일반 프로그램으로 쉽게 열어서 볼 수 있다\n",
    "\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 로 저장\n",
    "names.to_csv('sci_names_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSV 로 저장\n",
    "names.to_csv('sci_names_csv.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 엑셀로 저장\n",
    "import openpyxl\n",
    "scientists.to_excel('scientists_df_xlsx.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 연결\n",
    "# 분석하기 좋은 데이터는 분석하기 쉬운 상태로 가공 된 데이터\n",
    "# 실전 필드에서도 전처리 작업이 데이터 분석 작업의 80% 이상 차지\n",
    "\n",
    "# 분석하기 좋은 데이터의 기본 조건\n",
    "# 1. 변수는 열로 구성\n",
    "# 2. 값은 행으로 구성\n",
    "# 3. 열과 행으로 구성된 표 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('concat_1.csv')\n",
    "df2 = pd.read_csv('concat_2.csv')\n",
    "df3 = pd.read_csv('concat_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D\n",
      "0   a0   b0   c0   d0\n",
      "1   a1   b1   c1   d1\n",
      "2   a2   b2   c2   d2\n",
      "3   a3   b3   c3   d3\n",
      "0   a4   b4   c4   d4\n",
      "1   a5   b5   c5   d5\n",
      "2   a6   b6   c6   d6\n",
      "3   a7   b7   c7   d7\n",
      "0   a8   b8   c8   d8\n",
      "1   a9   b9   c9   d9\n",
      "2  a10  b10  c10  d10\n",
      "3  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "row_concat = pd.concat([df1, df2, df3])\n",
    "print(row_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    a3\n",
      "B    b3\n",
      "C    c3\n",
      "D    d3\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(row_concat.iloc[3, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B    C    D    0\n",
      "0   a0   b0   c0   d0  NaN\n",
      "1   a1   b1   c1   d1  NaN\n",
      "2   a2   b2   c2   d2  NaN\n",
      "3   a3   b3   c3   d3  NaN\n",
      "0  NaN  NaN  NaN  NaN   n1\n",
      "1  NaN  NaN  NaN  NaN   n2\n",
      "2  NaN  NaN  NaN  NaN   n3\n",
      "3  NaN  NaN  NaN  NaN   n4\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df1, new_row_series]))\n",
    "# NaN : 누락값/결측치\n",
    "# 시리즈가 새로운 열로 삽입 되었다\n",
    "\n",
    "# 시리즈를 데이터프레임에 넣으려고 하면 제대로 처리되지 않음\n",
    "# 열 이름이 없기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에 추가하려면 반드시 데이터프레임에 담아서 연결해야함\n",
    "new_row_df = pd.DataFrame([['n1', 'n2', 'n3', 'n4']], columns=['A', 'B', 'C', 'D'])\n",
    "print(new_row_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  n1  n2  n3  n4\n"
     ]
    }
   ],
   "source": [
    "# concat\n",
    "print(pd.concat([df1, new_row_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "0  n1  n2  n3  n4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/xfcnlqrd2nn2csm0b_0f87hc0000gn/T/ipykernel_3282/172915070.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  print(df1.append(new_row_df))\n"
     ]
    }
   ],
   "source": [
    "# append\n",
    "print(df1.append(new_row_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "4  n1  n2  n3  n4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_j/xfcnlqrd2nn2csm0b_0f87hc0000gn/T/ipykernel_3282/1368877906.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  print(df1.append(data_dict, ignore_index=True))\n"
     ]
    }
   ],
   "source": [
    "# ignore_index=True 기존 인덱스 무력화\n",
    "data_dict = {'A':'n1', 'B':'n2', 'C':'n3', 'D':'n4'}\n",
    "print(df1.append(data_dict, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D    A    B    C    D\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "# 열 방향으로 데이터 붙이기 axis=1\n",
    "col_concat = pd.concat([df1, df2, df3], axis=1)\n",
    "print(col_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   A    A\n",
      "0  a0  a4   a8\n",
      "1  a1  a5   a9\n",
      "2  a2  a6  a10\n",
      "3  a3  a7  a11\n"
     ]
    }
   ],
   "source": [
    "# 같은 이름의 열들만 추출\n",
    "print(col_concat['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D   A   B   C   D    A    B    C    D new_col_list\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8           n1\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9           n2\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10           n3\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11           n4\n"
     ]
    }
   ],
   "source": [
    "# 새로운 열을 간단히 추가\n",
    "col_concat['new_col_list'] = ['n1', 'n2', 'n3', 'n4']\n",
    "print(col_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7    8    9    10   11\n",
      "0  a0  b0  c0  d0  a4  b4  c4  d4   a8   b8   c8   d8\n",
      "1  a1  b1  c1  d1  a5  b5  c5  d5   a9   b9   c9   d9\n",
      "2  a2  b2  c2  d2  a6  b6  c6  d6  a10  b10  c10  d10\n",
      "3  a3  b3  c3  d3  a7  b7  c7  d7  a11  b11  c11  d11\n"
     ]
    }
   ],
   "source": [
    "print(pd.concat([df1, df2, df3], axis=1, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A   B   C   D\n",
      "0  a0  b0  c0  d0\n",
      "1  a1  b1  c1  d1\n",
      "2  a2  b2  c2  d2\n",
      "3  a3  b3  c3  d3\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# 공통 열과 공통 인덱스만 연결\n",
    "\n",
    "df1.columns = ['A', 'B', 'C', 'D'] \n",
    "df2.columns = ['E', 'F', 'G', 'H'] \n",
    "df3.columns = ['A', 'C', 'F', 'H']\n",
    "print(df1)\n",
    "print(type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2)\n",
    "print(type(df2))\n",
    "\n",
    "print(df3)\n",
    "print(type(df3))\n",
    "\n",
    "row_concat = pd.concat([df1, df2, df3]) \n",
    "print(row_concat)\n",
    "\n",
    "# 데이터 프레임의 공통 열들만 골라서 연결하면 결측치 가 생기지 않을 것이다. \n",
    "# 그러나 df1, df2, df3 모두의 공통열은 존재하지 않는다. \n",
    "# 만약에 공통열이 있다면 공통열만을 추출하는 명령은 join='inner'\n",
    "print(pd.concat([df1, df2, df3], join='inner'))\n",
    "\n",
    "print(pd.concat([df1,df3], ignore_index=False, join='inner'))\n",
    "\n",
    "# 행방향으로 연결 시....\n",
    "df1.index = [0, 1, 2, 3] \n",
    "df2.index = [4, 5, 6, 7] \n",
    "df3.index = [0, 2, 5, 7]\n",
    "\n",
    "print(df1)\n",
    "\n",
    "col_concat = pd.concat([df1, df2, df3], axis=1) \n",
    "print(col_concat)\n",
    "\n",
    "# merge 매서드 사용하여 데이터 연결하기\n",
    "\n",
    "person = pd.read_csv('survey_person.csv')\n",
    "site = pd.read_csv('survey_site.csv')\n",
    "survey = pd.read_csv('survey_survey.csv')\n",
    "visited = pd.read_csv('survey_visited.csv')\n",
    "\n",
    "\n",
    "print(person)\n",
    "print(site)\n",
    "\n",
    "visited_subset = visited.loc[[0, 2, 6], ]\n",
    "print(visited_subset)\n",
    "\n",
    "o2o_merge = site.merge(visited_subset, left_on='name', right_on='site') \n",
    "print(o2o_merge)\n",
    "\n",
    "site\n",
    "\n",
    "m2o_merge = site.merge(visited, left_on='name', right_on='site') \n",
    "print(m2o_merge)\n",
    "\n",
    "visited\n",
    "\n",
    "ps = person.merge(survey, left_on='ident', right_on='person') \n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken')\n",
    "\n",
    "print(ps)\n",
    "\n",
    "# 전처리 - 결측값(누락값, 결측치) 처리\n",
    "# NaN NAN nan 모두 가능하다. NaN을 선호한다. \n",
    "from numpy import NAN, NaN, nan \n",
    "\n",
    "# 결측치는 0이 아니다. ''도 아니다. 말 그대로 값이 없음을 나타낸다. \n",
    "# 따라서 '같다'도 사용할수 없다. \n",
    "\n",
    "print(NaN == True)\n",
    "print(NaN == False)\n",
    "print(NaN == 0)\n",
    "print(NaN == '')\n",
    "print(NaN == NaN)\n",
    "print(NaN == nan)\n",
    "print(NaN == NAN)\n",
    "print(nan == NAN)\n",
    "\n",
    "# NaN은 값 자체가 없기 때문에 자기 자신도 True 가 아니라 False 로 출력한다. \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "print(pd.isnull(NaN))\n",
    "print(pd.isnull(nan))\n",
    "print(pd.isnull(NAN))\n",
    "\n",
    "\n",
    "#  원래 결측치가 존재하는 파일을 불러오기도 하지만 결측치는 그 과정에서 - \n",
    "# 데이터 연결, 입력할때 등등에서 발생한다. \n",
    "\n",
    "visited = pd.read_csv('survey_visited.csv') \n",
    "survey = pd.read_csv('survey_survey.csv')\n",
    "\n",
    "print(visited)\n",
    "\n",
    "print(survey)\n",
    "\n",
    "vs = visited.merge(survey, left_on='ident', right_on='taken') \n",
    "print(vs)\n",
    "\n",
    "# 결측치의 개수 구하기\n",
    "\n",
    "# 전체 개수수\n",
    "ebola = pd.read_csv('country_timeseries.csv')\n",
    "print(ebola.count())\n",
    "\n",
    "num_rows = ebola.shape[0]\n",
    "num_missing = num_rows - ebola.count() \n",
    "print(num_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebola.isnull()\n",
    "\n",
    "ebola['Cases_Guinea'].isnull()\n",
    "\n",
    "print(ebola.Cases_Guinea.value_counts(dropna=False).head())\n",
    "\n",
    "print(ebola.Cases_Guinea.value_counts(dropna=False))\n",
    "\n",
    "# 결측치 처리\n",
    "\n",
    "# 1.임의의 값을 넣는다. (0,평균값, 최빈값)\n",
    "# 2.삭제한다. \n",
    "\n",
    "# 1.임의의 값(0) 으로 대체하는 방법\n",
    "print(ebola.fillna(0).iloc[0:10, 0:5])\n",
    "\n",
    "# 2.삭제한다. \n",
    "\n",
    "print(ebola.shape)\n",
    "\n",
    "ebola_dropna = ebola.dropna() \n",
    "print(ebola_dropna.shape)\n",
    "\n",
    "print(ebola_dropna)\n",
    "\n",
    "# 데이터 분석과 인공지능 분야에서는 결측치를 처리하는 능력이 매우매우 중요하다. \n",
    "# 결측치를 처리하는 방법은 반드시 숙지해야 한다.  \n",
    "\n",
    "# 자료형 변환\n",
    "\n",
    "import seaborn as sns   #  matplotlib() 보다 그림을 조금더 세련되게 그릴수 있다. \n",
    "\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "\n",
    "tips\n",
    "\n",
    "tips.dtypes\n",
    "\n",
    "tips['sex_str'] = tips['sex'].astype(str)\n",
    "print(tips.dtypes)\n",
    "\n",
    "tips['total_bill'] = tips['total_bill'].astype(str) \n",
    "print(tips.dtypes)\n",
    "\n",
    "tips['total_bill'] = tips['total_bill'].astype(float) \n",
    "print(tips.dtypes)\n",
    "\n",
    "# 이상한 데이터, 이상치, 잘못된 데이터를 변환\n",
    "# 정수가 있어야하는 열에 문자가 있으면 변환이 필요하다. \n",
    "tips_sub_miss = tips.head(10)\n",
    "tips_sub_miss\n",
    "\n",
    "tips_sub_miss.loc[[1, 3, 5, 7], 'total_bill'] = 'missing'\n",
    "\n",
    "tips_sub_miss\n",
    "\n",
    "print(tips_sub_miss.dtypes)\n",
    "#데이터에 문자가 포함되어 있어서 자료형 자체가 변경되었다. \n",
    "\n",
    "#tips_sub_miss['total_bill'].astype(float)\n",
    "#데이터 중에 글자가 있어서 실수형으로 변환이 불가능하다. \n",
    "\n",
    "#pd.to_numeric(tips_sub_miss['total_bill'])\n",
    "# 다른 방법을 시도해보았으나 역시 변환이 불가능하다. \n",
    "\n",
    "\n",
    "tips_sub_miss['total_bill'] = pd.to_numeric( tips_sub_miss['total_bill'], errors='coerce')\n",
    "\n",
    "print(tips_sub_miss.dtypes)\n",
    "\n",
    "# 옵션값을 coerce 로 설정하면 missing 값이 누락값으로 변경된다. \n",
    "\n",
    "# 카테고리 자료형\n",
    "\n",
    "# 판다스에는 유한한 범위 값만 가질수 있는 카테고리라는 특수한 자료형이 있다. \n",
    "# 만약에 5개의 언어만 저장할 열이 있다고 생각한다면 문자열 자료형보다는\n",
    "# 카테고리 자료형을 사용하는 것이 용량과 속도면에서도 훨씬 효율적이다. \n",
    "\n",
    "# 1. 용량과 속도면에서 탁월하다. \n",
    "# 2. 주로 동일한 문자열이 반복되어 데이터를 구성하는 경우 사용된다. \n",
    "\n",
    "\n",
    "\n",
    "tips['sex'] = tips['sex'].astype('str') \n",
    "print(tips.info())\n",
    "\n",
    "tips['sex'] = tips['sex'].astype('category') \n",
    "print(tips.info())\n",
    "\n",
    "tips['sex'] = tips['sex'].astype('str') \n",
    "tips['smoker'] = tips['smoker'].astype('str') \n",
    "tips['day'] = tips['day'].astype('str') \n",
    "tips['time'] = tips['time'].astype('str') \n",
    "print(tips.info())\n",
    "\n",
    "#groupby\n",
    "\n",
    "import pandas as pd \n",
    "df = pd.read_csv('gapminder.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "gdf_dict = df.groupby('year').agg({'lifeExp': 'mean', 'pop': 'median', 'gdpPercap': 'median'}) \n",
    "print(gdf_dict)\n",
    "\n",
    "# 타이타닉 \n",
    "\n",
    "# 1. 전처리\n",
    "# 2. 데이터분석\n",
    "# 3. 인공지능(머신러닝)\n",
    "# 4. 예측 (디카프리오, 21, 남성, 3등석 ? 생존률)\n",
    "#         (윈슬렛, 18, 여성, 1등석 ? 생존률)\n",
    "\n",
    "# 연예인 닮은 꼴 찾기 API\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
